
# -- --------------------------------------------------------------------------------------------------- -- #
# -- Project: lucidmode                                                                                  -- #
# -- Description: A Lightweight Framework with Transparent and Interpretable Machine Learning Models     -- #
# -- changelog.txt: A text file with notes on planning and progress on changes                           -- #
# -- Author: IFFranciscoME - if.francisco.me@gmail.com                                                   -- #
# -- License: GPL-3.0 License                                                                            -- #
# -- Repository: https://github.com/lucidmode/lucidmode                                                  -- #
# -- --------------------------------------------------------------------------------------------------- -- #

## TODO: v0.4.1-beta1.0

**** DONE ****

#### Functionality

Optimizers
- Modify Gradient Descent to support other two options
- Add Stochastic Gradient Descent (batch_size == 1)
- Add Batch Gradient Descent (1 < batch_size < n_sample)

#### Changelog

- Modification: Rename Sequential class as NeuralNet
- Documentation: Renamed private methods according to python official docs: 
	https://docs.python.org/3/tutorial/classes.html#private-variables

*** PENDING ****

- Enhacement: Complete all the docstrings in reStructured text of functional and planned methods.
- Enhacement: Write README file with srt sintax

- Fix: ReLU activation for hidden layers

- Addition: Load/Save weights of NeuralNet object
- Addition: Load/Save topology, weights, history, of NeuralNet object
- Addition: L1, L2, ElasticNet default values

- Enhacement: Create Optimizer class with SGD and LM and port SGD from fit method
- Enhacement: Add Levenberg-Marquardt Algorithm for weights optimization
- Enhacement: Add ignore_warnings flag for cases of RuntimeWarning in math operations
			  divide by zero, very large weights

- Test: SGD with different batch_size. 
- Test: SGD with different combination of activation functions in layers. 
- Test: SGD with weight initialization options
- Test: SGD with sigmoid and Softmax activation functions in output layer
- Test: SGD with no validation data (only train data)
- Test: SGD with with different configurations and values of L1, L2, ElasticNet.

GLOBAL TASKS:
- Build documentation with ReadTheDocs
- Add badges to README.rst
- Perform UnitTests
- Build minimal Landing Page
- Build first beta version in pypi.org
- Hire fiverr designer for logo(github)
